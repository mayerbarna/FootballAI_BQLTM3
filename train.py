import gfootball.env as football_env
import keras.backend as keras
from keras.layers import Input, Dense
from keras.models import Model
from keras.optimizers import Adam
import numpy as np

env = football_env.create_environment(env_name='academy_counterattack_hard', representation='simple115v2', render=True)

state = env.reset()

state_dimension = env.observation_space.shape
n_actions = env.action_space.n



print(state_dimension)
print(n_actions)


states = []
actions = []
values = []  # generated by the critic model
masks = []  # used to check if the game is completed
rewards = []

# defined the actor model to give us an action depending on the current state instead of a random one

def get_ppo_actor_model_from_simple(input_dimensions, output_dimensions):
    state_input_shape = Input(shape=input_dimensions)   # define the input shape from the simple data
    advantages = Input(shape=(1, 1,))
    rewards = Input(shape=(1, 1,))
    values = Input(shape=(1, 1,))
    oldpolicy_probs = Input(shape=(1, output_dimensions,))

    # Classification block
    x = Dense(512, activation='relu', name='fc1')(state_input_shape)
    x = Dense(256, activation='relu', name='fc2')(x)
    output_actions = Dense(n_actions, activation='softmax', name='predictions')(x)

    # simple representation of the game:
        #22 - (x,y) coordinates of left team players
        #22 - (x,y) direction of left team players
        #22 - (x,y) coordinates of right team players
        #22 - (x, y) direction of right team players
        #3 - (x, y and z) - ball position
        #3 - ball direction
        #3 - one hot encoding of ball ownership (noone, left, right)
        #11 - one hot encoding of which player is active
        #7 - one hot encoding of game_mode

    #model input --> miert kell bele az input shapen kivul a tobbi?
    model = Model(inputs=[state_input_shape, oldpolicy_probs, advantages, rewards, values],
                  outputs=[output_actions])

    model.compile(optimizer=Adam(lr=1e-4), loss='mse')
    return model

#PPO

dummy_n = np.zeros((1, 1, n_actions))
dummy_1 = np.zeros((1, 1, 1))

actor_model = get_ppo_actor_model_from_simple(input_dimensions=state_dimension, output_dimensions=n_actions)

steps_of_ppo = 128
for _ in range(steps_of_ppo):
    input_tensor = keras.expand_dims(state,0)
    distribution_of_actions = actor_model.predict([input_tensor, dummy_n, dummy_1, dummy_1, dummy_1], steps=1 )

    executable_action = np.random.choice(n_actions, p=distribution_of_actions[0,:]) # select a random action according to the calculated distribution

    action_onehot = np.zeros(n_actions)
    action_onehot[executable_action] = 1
    observation, reward, is_done, information =  env.step(executable_action)
    if is_done:
        env.reset()

env.close()